<!--
%\VignetteIndexEntry{Introduction to corpus}
%\VignetteEngine{knitr::rmarkdown}
%\VignetteEncoding{UTF-8}
-->

Introduction to corpus
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "corpus-", fig.retina = TRUE)
options(width = 100)
```

Overview
--------

This vignette demonstrates the functionality provided by the *corpus* R
package. The running example throughout is an analysis of the text of L. Frank
Baum's novel, *The Wonderful Wizard of Oz*.


Setup
-----

We load the *corpus* package, set the color palette, and set the random number
generator seed.  We will not use any external packages in this vignette.

```{r}
library("corpus")

# colors from RColorBrewer::brewer.pal(6, "Set1")
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33"))

# ensure consistent runs
set.seed(0)
```


Data preparation
----------------

The *The Wonderful Wizard of Oz* is available as
[Project Gutenberg EBook #55][gutenberg-oz]. We first download the text and
strip off the Project Gutenberg header and footer.

```{r}
url <- "http://www.gutenberg.org/cache/epub/55/pg55.txt"
raw <- readLines(url, encoding = "UTF-8")

# the text starts after the Project Gutenberg header...
start <- grep("^\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK", raw) + 1

# ...end ends at the Project Gutenberg footer.
stop <- grep("^End of Project Gutenberg", raw) - 1

lines <- raw[start:stop]
```

The novel starts with front matter: a title page, table of contents,
introduction, and half title page. Then, a series of chapters follow.
We group the lines by section.

```{r}
# the front matter ends at the half title page
half_title <- grep("^THE WONDERFUL WIZARD OF OZ", lines)

# chapters start with "1.", "2.", etc...
chapter <- grep("^[[:space:]]*[[:digit:]]+\\.", lines)

# ... and appear after the half title page
chapter <- chapter[chapter > half_title]

# get the section texts (including the front matter)
start <- c(1, chapter + 1) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), start, end)

# trim leading and trailing white space
text <- trimws(text)

# discard the front matter
text <- text[-1]

# get the section titles, removing the prefix ("1.", "2.", etc.)
title <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
title <- trimws(title)
```


Corpus object
-------------

Now that we have obtained our raw data, we put everything together into a
corpus object, constructed via the `corpus` function:

```{r}
data <- corpus(title, text)

# set the row names; not necessary but makes results easier to read
rownames(data) <- sprintf("ch%02d", seq_along(chapter))
```

The `corpus` function behaves similarly to the `data.frame` function, but
expects one of the columns to be named `"text"`. Note that we do not need
to specify `stringsAsFactors = FALSE` when creating a corpus object.
As an alternative to using the `corpus` function, we can construct a data
frame using some other method (e.g., `read.csv` or `read_ndjson`) and use
the `as_corpus` function.


A corpus object is just a data frame with a column named "text" of type
`"corpus_text"`. When using the *corpus* library, it is not strictly necessary
to use corpus objects as inputs; most functions will accept with character
vectors and ordinary data frames. Using a corpus object gives better
printing behavior and allows setting a `text_filter` attribute to override
the default text preprocessing.

```{r}
print(data) # better output than printing a data frame, cuts off after 20 rows
print(data, 5) # cuts off after 5 rows
print(data, -1) # prints all rows
```


Tokenization
------------

Text in *corpus* is represented as a sequence of tokens, each taking a value
in a set of types. We can see the tokens for one or more elements using
the `text_tokens` function:

```{r}
text_tokens(data["ch24",]) # Chapter 24's tokens
```

The default behavior is to normalize tokens by changing the cases of the
letters to lower case. A `text_filter` object controls the rules for
segmentation and normalization. We can inspect the text filter:

```{r}
text_filter(data)
```

We can change the text filter properties:

```{r}
text_filter(data)$map_case <- FALSE
text_filter(data)$drop_punct <- TRUE
text_tokens(data["ch24",])
```

To restore the defaults, set the text filter to `NULL`:

```{r}
text_filter(data) <- NULL
```

In addition to mapping case and quotes (the defaults), I'm going to drop
punctuation.

```{r}
text_filter(data) <- text_filter(drop_punct = TRUE)
```

The tokenizer allows for precise controlling over token dropping and token
stemming. It also allows combining two or more words into a single token as
in the following example:

```{r}
text_tokens("I live in New York City, New York",
            filter = text_filter(combine = c("new york", "new york city")))
```

This example using the optional second argument to `text_tokens` to override
the first argument's default text filter.  Here, instances of "new york" and
"new york city" get replaced by single tokens, with the longest match taking
precedence. See the documentation for `text_tokens` describes the full
tokenization process.


Texts as sequences
------------------

The mental model of the *corpus* package is that a text is s sequence of
tokens, some of which are droped (`NA`). Every object has a `text_filter()`
property defining its token boundaries. The default token filter transforms
the text to Unicode composed normal form (NFC), applies Unicode case folding,
and maps curly quotes to straight quotes. Text objects, created with `as_text`
or `as_corpus` can have custom text filters. You cannot set the text filter
for a character vector. However, all *corpus* text functions accept a `filter`
argument to override the input object's text filter (this is demonstrated in
the "New York City" example in the previous section).


To find out the length of a text, as measured in dropped and non-dropped
tokens, use the `text_length` function.


```{r}
text_tokens("One, two, three!", filter = text_filter(drop_punct = TRUE))
text_length("One, two, three!", filter = text_filter(drop_punct = TRUE))
```

You can set subsequences of consecutive tokens using the `text_sub` function.
This function accepts two arguments specifying the start and then end token
position. The following example extracts the subsequences from
positions 2 to 4:

```{r}
text_sub(c("One, two, three!", "4 5 6 7 8 9 10"), 2, 4,
         filter = text_filter(drop_punct = TRUE))
```

Negative indices count from the end of the sequence, with `-1` denoting the
last token.

```{r}
# last 2 tokens
text_sub(c("One, two, three!", "4 5 6 7 8 9 10"), -2, -1,
         filter = text_filter(drop_punct = TRUE))
```

Note that `text_length` and `text_sub` count both dropped and non-dropped
tokens.


Here's how to get the last 10 tokens in each chapter:

```{r}
text_sub(data, -10)
```

In this example, we do not specify the ending position, so it defaults to `-1`.



Text statistics
---------------

### Token, type, and sentence counts

The `text_ntoken`, `text_ntype`, and `text_nsentence` functions return the
numbers of non-dropped tokens, unique types, and sentences, respectively, in a set of
texts. We can use these functions to get an overview of the section lengths
and lexical diversities.

```{r}
text_ntoken(data)
text_ntype(data)
text_nsentence(data)
```

The `text_stats` function computes all three counts and presents the results
in a data frame:

```{r}
stats <- text_stats(data)
print(stats, -1) # print all rows instead of truncating at 20
```

We can see that the last chapter is the shortest, with 74 tokens, 56 unique
types, and 8 sentences. Chapter 12 is the longest.


### Application: Testing Heaps' law

[Heaps' law][heaps-law] says that the logarithm of the number of unique
types is a linear function of the number of tokens. We can test this law
formally with a regression analysis.

In this analysis, we will exclude the last chapter (Chapter 24), because it is
much shorter than the others and has a disproportionate influence on the fit.

```{r}
subset <- row.names(stats) != "ch24"
model <- lm(log(types) ~ log(tokens), stats, subset)
summary(model)
```

We can also inspect the relation visually
```{r heapslaw, fig.width = 12, fig.cap = "Heaps' Law"}
par(mfrow = c(1, 2))
plot(log(types) ~ log(tokens), stats, col = 2, subset = subset)
abline(model, col = 1, lty = 2)

plot(log(stats$tokens[subset]), rstandard(model), col = 2,
     xlab = "log(tokens)")
abline(h = 0, col = 1, lty = 2)

outlier <- abs(rstandard(model)) > 2
text(log(stats$tokens)[subset][outlier], rstandard(model)[outlier],
     row.names(stats)[subset][outlier], cex = 0.75, adj = c(-0.25, 0.5),
     col = 2)
```

The analysis tells us that Heap's law accurately characterizes the lexical
diversity (type-to-token ratio) for the main chapters in *The Wizard of Oz*.
The number of unique types grows roughly as the number of tokens raised to the
power `0.6`.


The one chapter with an unusually low lexical diversity is Chapter 16. This
chapter contains mostly dialogue between Oz and Dorothy's simple-minded
companions (the Scarecrow, Tin Woodman, and Lion).


Term statistics
---------------

### Counts and prevalence

We get term statistics using the `term_stats` function:

```{r}
term_stats(data)
```

This returns a data frame with each row giving the count and support for each
term. The "count" is the total number of occurrences of the term in the
corpus. The "support" is the number of texts containing the term. In the
output above, we can see that "the" is the most common term, appearing 2922
times total in all 24 chapters. The pronoun "her" is the 20th most common
term, appearing in all but one chapter.


The most common words are function words, commonly known as "stop" words. We
can exclude these terms from the tally using the `subset` argument.

```{r}
term_stats(data, subset = !term %in% stopwords("english"))
```

The character names "dorothy", "toto", and "scarecrow" show up at the top of
the list of the most common terms.


### Higher-order n-grams

Beyond searching for single-type terms, we can also search for multi-type
terms ("n-grams").

```{r}
term_stats(data, ngrams = 5)
```

The `types` argument allows us to request the component types in the result:

```{r}
term_stats(data, ngrams = 3, types = TRUE)
```

Here are the most common 2-, 3-grams starting with "dorothy", where the second
type is not a function word
```{r}
term_stats(data, ngrams = 2:3, types = TRUE,
           subset = type1 == "dorothy" & !type2 %in% stopwords("english"))
```


Searching for terms
-------------------

Now that we have identified common terms, we might be interested in seeing
where they appear. For this, we use the `text_locate` function.

Here are all instances of the term "dorothy looked":

```{r}
text_locate(data, "dorothy looked")
```

Note that we match against the type of the token, not the raw token itself, so
we are able to detect capitalized "Dorothy". This is especially useful when we
want to search for a stemmed token. Here are all instances of tokens that
stem to "scream":

```{r}
text_locate(data, "scream", filter = text_filter(stemmer = "english"))
```

If we would like, we can search for multiple phrases at the same time:

```{r}
text_locate(data, c("wicked witch", "toto", "oz"))
```

We can also request that the results be returned in random order. This is
useful for inspecting a random sample of the matches:

```{r}
text_locate(data, c("wicked witch", "toto", "oz"), random = TRUE)
```

Other functions allow counting term occurrences, testing for whether a term
appears in a text, and getting the subset of texts containing a term:

```{r}
text_count(data, "the great oz")
text_detect(data, "the great oz")
text_subset(data, "the great oz")
```


Segmenting text
---------------

### Sentences and blocks of tokens

*Corpus* can split text into blocks of sentences or tokens using the
`text_split` function. By default, this function splits into sentences. Here,
for example, are the last 10 sentences in the book:

```{r}
tail(text_split(data), 10)
```

The result of `text_split` is a data frame, with one row for each segment
identifying the parent text (as a factor), the index of the segment in the
parent text (an integer), and the segment text.


The second argument to `text_split` specifies, the units, "sentences" or
"tokens". The third argument specifies the maximum segment size, defaulting
to one. Each text gets divided into approximately equal-sized segments, with
no segment being larger than the specified size.


Here is an example of splitting two texts into segments of size at most four
tokens.

```{r}
text_split(c("the wonderful wizard of oz", paste(LETTERS, collapse = " ")),
           "tokens", 4)
```

### Application: Witch tracking

We can combine `text_split` with `text_count` to measure the occurrences rates
for the term "witch" over the course of the novel.  Here, the chunks have
varying sizes, so we look at the rates rather than the raw counts.


```{r witch-occurences, fig.width = 12, fig.cap = "'witch' Occurences"}
chunks <- text_split(data, "tokens", 500)
size <- text_ntoken(chunks)

unit <- 1000 # rate per 1000 tokens
count <- text_count(chunks, "witch")
rate <-  count / size * unit

i <- seq_along(rate)
plot(i, rate, type = "l", xlab = "Segment",
     ylab = "Rate \u00d7 1000",
     main = paste(dQuote("witch"), "Occurrences"), col = 2)
points(i, rate, pch = 16, cex = 0.5, col = 2)
```

We can see Dorothy's house landing on the Wicked Witch of the East in the and
the subsequent fallout in the beginning of the novel. Around segment 40, we
see the events surrounding Dorothy's battle with the Wicked Witch of the West.
At the end of the novel, we see the Good Witch of the South appearing to help
Dorothy get home.


Term frequency matrix
---------------------

Many downstream text analysis tasks require tabulating a matrix of text-term
occurence counts. We can get such a matrix using the `term_matrix` function:

```{r}
x <- term_matrix(data)
dim(x)
```

This function returns a sparse matrix object from the *Matrix* package. In the
default usage, the rows of the matrix correspond to texts, and the columns
correspond to terms. For a "term-by-document" matrix, you can use the
`transpose` option:

```{r}
xt <- term_matrix(data, transpose = TRUE)
```

Alternatively, to get a data frame of the (text, term, count) tripples, use
ther `term_frame` function:

```{r}
(xf <- term_frame(data))
```

You can include n-grams in the result if you would like:
```{r}
x3 <- term_matrix(data, ngrams = 1:3) # 1-, 2-, and 3-grams
```

Or, you can specify the columns to include in the matrix

```{r}
(x <- term_matrix(data, select = c("dorothy", "toto", "wicked witch", "the great oz")))
```

The columns of `x` will be in the same order as specified by the `select`
argument. Note that we can request higher-order n-grams.


Emotion lexicon
---------------

*Corpus* provides a lexicon of terms connoting emotional affect, the
[WordNet Affect Lexicon][wn-affect].

```{r}
wnaffect
```

This lexicon classifies a large set of terms correlated with emotional affect
into four main categories: "Positive", "Negative", "Ambiguous", and "Neutral",
and a variety of sub-categories. Here is a summary:

```{r}
summary(wnaffect)
```

Here are the term counts broken down by category:

```{r}
with(wnaffect, table(category, emotion))
```

Terms can appear in multiple categories, or with multiple parts of speech.

```{r}
# some duplicate terms
subset(wnaffect, term %in% c("caring", "chill", "hopeful"))
```

The term "chill", for example, is listed as denoting both positive calmness
and negative fear, among other emotional affects.


Application: Emotion in _The Wizard of Oz_
------------------------------------------

### Overview

For our final application, we will track emotion word usage over the course of
_The Wizard of Oz_. We will do this by segmenting the novel into small chunks,
and then measure the occurrence rates of emotion words in these chunks.


### Lexicon

We will first need a lexicon of emotion words. We will take as a starting
point the WordNet-Affect lexicon, but we will remove "Neutral" emotion words.

```{r}
affect <- subset(wnaffect, emotion != "Neutral")
affect$emotion <- droplevels(affect$emotion) # drop the unused "Neutral" level
affect$category <- droplevels(affect$category) # drop unused categories
```

Rather than blindly applying the lexicon, we first check to see what the most
common emotion terms are.

```{r}
term_stats(data, subset = term %in% affect$term)
```

A few terms jump out as unusual: "yellow" is probably for the yellow brick
road, and "wicked" is probably for the wicked witch. When these terms appear,
they probably don't describe an emotional state. We can verify this using the
`text_locate` function, which shows these terms in context.

```{r}
text_locate(data, "yellow", random = TRUE)
text_locate(data, "wicked", random = TRUE)
```

Here, we use the `random = TRUE` option to return the matches in random order.
Since we are only looking at a subset of the matches, we use this option to
ensure that we don't make conclusions about these words using a biased sample.
With the default option (`random = FALSE`), we would only see the matches at
the beginning of the novel.


We can also inspect the first token after each appearance of "wicked":

```{r}
term_stats(text_sub(text_locate(data, "wicked")$after, 1, 1))
```

of the 72 appearances of "wicked", 58 are followed by "witch" or "witches".
Likewise, "yellow" is often referring to the road, or referring to the color
of an object and not an emotion:

```{r}
term_stats(text_sub(text_locate(data, "yellow")$after, 1, 1))
```

The word "heart" is also suspiciously frequent. Here are some occurrences of
that word:

```{r}
text_locate(data, "heart", random = TRUE)
```

A central part of the novel's plot is the Tin Woodman's quest for a heart; it
is not surprising that the word "heart" shows up so frequently. Indeed, most
appearances of the word "heart" are within 25 tokens of "woodman":

```{r}
loc <- text_locate(data, "heart")
before <- text_detect(text_sub(loc$before, -25, -1), "woodman")
after <- text_detect(text_sub(loc$after, 1, 25), "woodman")
summary(before | after)
```
"Woodman" appears within 25 tokens of "heart" in in 45 of the 67
contexts where the latter word appears.

All of this analysis shows that we should probably exclude these terms if we
are interested in words that connote emotion in _The Wizard of Oz_. We will
also drop some other terms from the lexicon based on similar considerations
(not shown here):

```{r}
affect <- subset(affect, !term %in% c("drop", "heart", "wicked", "yellow", "blue"))
```

### Term emotion matrix

Now that we have a lexicon, our plan is to segment the text into smaller
chunks and then compute the emotion occurrence rates in each chunk, broken
down by category ("Positive", "Negative", or "Ambigous").

To facilitate the rate computations, we will form a term-by-emotion rate for
the lexicon:

```{r}
term_scores <- with(affect, unclass(table(term, emotion)))
head(term_scores)
```

Here, `term_scores` is a matrix with entry *(i,j)* indicating the number of
times that term *i* appeared in the affect lexicon with emotion *j*.

We re-classify any term appearing in two or more categories as ambigous:

``{r}
ncat <- rowSums(term_scores > 0)
term_scores[ncat > 1, c("Positive", "Negative", "Ambiguous")] <- c(0, 0, 1)
```

At this point, every term is in one category, but the score for the term could
be 2, 3, or more, depending on the number of sub-categories the term appeared
in. We replace these larger values with one.

```{r}
term_scores[term_scores > 1] <- 1
```


### Segmenting chapters into smaller chunks

To compute emotion occurence rates, we start by splitting each chapter into
equal-sized segments of at most 500 tokens. The specific size of 500 tokens is
somewhat arbitrary, but not entirely so. We want the segments to be large enough
so that our rate estimates are reliable, but not so large that the emotion
usage is heterogeneous within the segment.

```{r}
chunks <- text_split(data, "tokens", 500)
```

Within a chapter, the segments all have approximately the same size. However,
since the chapters have different lengths, there is some variation in segment
size across chapters:

```{r}
(n <- text_ntoken(chunks))
```

(If we wanted equal sized segments, we could have concatenated the chapters
together and then split the combined text. The disadvantage of this approach
is that some segments would be split across multiple chapters.) 


### Computing emotion rates

For the count of each emotion category in each segment, we form a text-by-term
matrix of counts, and then multiply this by the term-by-emotion score matrix.

```{r}
x <- term_matrix(chunks, select = rownames(term_scores))
text_scores <- x %*% term_scores
```

For the occurence rates, we divide the counts by the segment sizes. We then
multiply by 1000 so that rates are given as occurences per 1000 tokens.

```{r}
# compute the rates per 1000 tokens
unit <- 1000
rate <- list(pos = text_scores[, "Positive"] / n * unit,
             neg = text_scores[, "Negative"] / n * unit,
             ambig = text_scores[, "Ambiguous"] / n * unit)
rate$total <- rate$pos + rate$neg + rate$ambig
```

We use the binomial variance formula to get the standard errors:

```{r}
# compute the standard errors
se <- lapply(rate, function(r) sqrt(r * (unit - r) / n))
```

This is a crude estimate that makes some independence assumptions, but it
gives a reasonable approximation of the uncertainty associated with our
measured rates.



### Plotting the results

We plot the four rate curves as time series. Our main focus is on the total
emotion usage. For this curve, we also put a horizonal dashed line at its
mean, and we indicating the "interesting" segments, those that appear more
than two standard deviations away from the main, by putting error bars on
these points.


```{r emotion, fig.width = 12, fig.cap = "Emotion in Oz"}
# set up segment IDs
i <- seq_len(nrow(chunks))

# set the plot margins, with extra space below the plot
par(mar = c(4, 4, 11, 9) + 0.1, las = 1)

# set up the plot coordinates; put labels but no axes
xlim <- range(i - 0.5, i + 0.5)
ylim <- range(0, rate$total + se$total, rate$total - se$total)
plot(xlim, ylim, type = "n", xlab = "Segment", ylab = "Rate \u00d7 1000", axes = FALSE,
     xaxs = "i")
usr <- par("usr") # get the user coordinates for later

# put tick marks at multiples of 5 on the x axis; labels at multiples of 10
axis(1, at = i[i %% 5 == 0], labels = FALSE)
axis(1, at = i[i %% 10 == 0], labels = TRUE)

# defaults for the y axis
axis(2)

# put vertical lines at chapter boundaries
abline(v = tapply(i, chunks$parent, min) - 0.5, col = "gray")

# put chapter titles above the plot
labels <- data$title
at <- tapply(i, chunks$parent, mean)

# (adapted from https://www.r-bloggers.com/rotated-axis-labels-in-r-plots/)
text(at, usr[4] + 0.01 * diff(usr[3:4]),
     labels = labels, adj = 0, srt = 45, cex = 0.8, xpd = TRUE)

# frame the plot
box()

# colors for the different emotions, from RColorBrewer::brewer.pal(3, "Set2")
col <- c(total = "#000000", pos = "#FC8D62", neg = "#8DA0CB", ambig = "#66C2A5")

# add a legend on the right hand side
legend(usr[2] + 0.02 * diff(usr[1:2]), usr[3] + 0.8 * diff(usr[3:4]),
       legend = c("Total", "Positive", "Negative", "Ambiguous"),
       title = expression(bold("Emotion")),
       fill = col[c("total", "pos", "neg", "ambig")],
       cex = 0.8, xpd = TRUE)

# for the total rate, put a dashed line at the mean rate
abline(h = mean(rate$total), lty = 2, col = col[["total"]])

# plot each rate type
for (t in c("ambig", "neg", "pos", "total")) {
    r <- rate[[t]]
    s <- se[[t]]
    cl <- col[[t]]

    # add lines and points
    lines(i, r, col = cl)
    points(i, r, col = cl, pch = 16, cex = 0.5)

    # for the total, put standard errors around interesting points
    if (t == "total") {
        # "interesting" defined as rate >2 sd away from mean
        int <- abs((r - mean(r)) / sd(r)) > 2

        segments(i[int], (r - s)[int], i[int], (r + s)[int], col = cl)
        segments((i - .2)[int], (r - s)[int], (i + .2)[int], (r - s)[int], col = cl)
        segments((i - .2)[int], (r + s)[int], (i + .2)[int], (r + s)[int], col = cl)
    }
}
```

### Discussion

This is a crude measurement, but it appears to give a reasonable approximation
of the emotional dynamics of the novel. The "Positive", "Negative", and
"Ambiguous" categories are hard to interpet, but "Total" emotion seems to make
sense.  The novel starts slowly, then quickly gets exciting when the tornado
hits Kansas. There is a lull while Dorothy is in Munchkinland, but gets
exciting when Dorothy meets the Cowardly Lion in the forest. The excitement
level ebbs and flows throughout the rest of the novel.


The least exciting segment appears in "The Queen of the Field Mice", a chapter
that got cut from the _Wizard of Oz_ movie.There are two segments with
particularly high emotion: segment 45 from "The Wonderful City of Oz", where
the Tin Woodman and the Cowardly Lion ask the Wizard for help, and he compels
them to defeat the Wicked Witch of the West; and segment 64, from "The
Discovery of Oz, the Terrible", where Dorothy and her companions discover that
the Wizard of Oz is not a wizard at all, just a common man.


Summary
-------

The *corpus* library provides facilities for transforming texts into sequences
of tokens and for computing the statistics of these sequences. The
`text_filter()` function allows us to control the transformation from text to
tokens. The `text_stats()` and `term_stats()` functions compute text- and
term-level occurrence statistics. The `text_locate()` function and allow us to
search for terms within texts. The `term_matrix()` function computes a
text-by-term frequency matrix. These functions and their variants provide the
building blocks for analyzing text.

For more information, check the other vignettes or the package documentation
with `library(help = "corpus")`.


[gutenberg-oz]: http://www.gutenberg.org/ebooks/55 "The Wonderful Wizard of Oz"
[heaps-law]: https://en.wikipedia.org/wiki/Heaps%27_law "Heap's law"
[wn-affect]: http://wndomains.fbk.eu/wnaffect.html "WordNet-Affect"
